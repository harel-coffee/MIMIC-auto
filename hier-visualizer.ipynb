{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "sys.argv.append('--dynet_mem')\n",
    "sys.argv.append('5000')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "import _gdynet as dy\n",
    "dy.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = '2'\n",
    "with open('mimic/'+v+'.p',mode='rb') as fp:\n",
    "    x,y,all_words,all_labels = pickle.load(fp)\n",
    "\n",
    "    \n",
    "desc_dict = {}\n",
    "for line in open('mimic/2/ICD9_descriptions'):\n",
    "    line = line.strip().split('\\t')\n",
    "    desc_dict[line[0]] = line[1]\n",
    "    \n",
    "train_set = list(zip(x[:-2282], y[:-2282]))\n",
    "test_set = list(zip(x[-2282:], y[-2282:]))\n",
    "VOCAB_SIZE = len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings_size = 50\n",
    "word_gru_layers = 1\n",
    "word_gru_state_size = 50\n",
    "sent_gru_layers = 1\n",
    "sent_gru_state_size = 50\n",
    "\n",
    "model = dy.Model()\n",
    "\n",
    "embeddings = model.add_lookup_parameters((VOCAB_SIZE, embeddings_size))\n",
    "word_gru_builder = dy.GRUBuilder(word_gru_layers, embeddings_size, word_gru_state_size, model)\n",
    "word_attention_w1 = model.add_parameters((word_gru_state_size, word_gru_state_size))\n",
    "#word_w2 =  \n",
    "word_attention_v = model.add_parameters((1, word_gru_state_size))\n",
    "\n",
    "sent_gru_builder = dy.GRUBuilder(sent_gru_layers, word_gru_state_size, sent_gru_state_size, model)\n",
    "\n",
    "sent_attention_w1s = []\n",
    "sent_attention_vs = []\n",
    "classifier_ws = []\n",
    "classifier_bs = []\n",
    "\n",
    "for _ in all_labels:\n",
    "    sent_attention_w1s.append(model.add_parameters((sent_gru_state_size, sent_gru_state_size)))\n",
    "    #word_w2 =  \n",
    "    sent_attention_vs.append(model.add_parameters((1, sent_gru_state_size)))\n",
    "    \n",
    "    classifier_ws.append(model.add_parameters((2, sent_gru_state_size)))\n",
    "    classifier_bs.append(model.add_parameters((2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_probs(doc):\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    encoded_sents = []\n",
    "    for sent in doc:\n",
    "        #e = dy.parameter(embeddings)\n",
    "        embedded_sent = [embeddings[word] for word in sent]\n",
    "\n",
    "        states =  word_gru_builder.initial_state().add_inputs(embedded_sent)\n",
    "        rnn_outputs = [s.output() for s in states]\n",
    "        \n",
    "        w = dy.parameter(word_attention_w1)\n",
    "        v = dy.parameter(word_attention_v)\n",
    "\n",
    "        attention_weights = [v*dy.tanh(w*o) for o in rnn_outputs]\n",
    "        attention_weights = dy.softmax(dy.concatenate(attention_weights))\n",
    "        \n",
    "        output_vector = dy.esum(\n",
    "            [vector * attention_weight for vector, attention_weight in zip(rnn_outputs, attention_weights)])\n",
    "        encoded_sents.append(output_vector)\n",
    "    \n",
    "    states = sent_gru_builder.initial_state().add_inputs(encoded_sents)\n",
    "    rnn_outputs = [s.output() for s in states]\n",
    "    all_probs = []\n",
    "    for i in range(len(all_labels)):\n",
    "        w = dy.parameter(sent_attention_w1s[i])\n",
    "        v = dy.parameter(sent_attention_vs[i])\n",
    "        attention_weights = [v*dy.tanh(w*o) for o in rnn_outputs]\n",
    "        attention_weights = dy.softmax(dy.concatenate(attention_weights))\n",
    "        \n",
    "        output_vector = dy.esum(\n",
    "            [vector * attention_weight for vector, attention_weight in zip(rnn_outputs, attention_weights)])\n",
    "        \n",
    "        w = dy.parameter(classifier_ws[i])\n",
    "        b = dy.parameter(classifier_bs[i])\n",
    "        probs = dy.softmax(w * output_vector + b)\n",
    "        all_probs.append(probs)\n",
    "    return all_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(model, train_set, epochs = 20):\n",
    "    def get_loss(probs, trues):\n",
    "        return dy.esum([-dy.log(dy.pick(prob, true)) for prob, true in zip(probs, trues)])\n",
    "    \n",
    "    trainer = dy.AdamTrainer(model)\n",
    "    for e in range(epochs):\n",
    "        losses = []\n",
    "        for i, training_example in enumerate(train_set):\n",
    "            doc, labels = training_example\n",
    "\n",
    "            loss = get_loss(get_probs(doc), labels)\n",
    "            losses.append(loss.value())\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "\n",
    "            # Accumulate average losses over training to plot\n",
    "            if i%(int(len(train_set)/100)) == 0:\n",
    "                print('!',np.mean(losses), end='')\n",
    "                losses = []\n",
    "        print('epoch', e, 'done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train(model, train_set, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f35745a9bb51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-f35745a9bb51>\u001b[0m in \u001b[0;36meval\u001b[0;34m(model, test_set)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0159da984b91>\u001b[0m in \u001b[0;36mget_probs\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         output_vector = dy.esum(\n\u001b[0;32m---> 32\u001b[0;31m             [vector * attention_weight for vector, attention_weight in zip(rnn_outputs, attention_weights)])\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_ws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0159da984b91>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         output_vector = dy.esum(\n\u001b[0;32m---> 32\u001b[0;31m             [vector * attention_weight for vector, attention_weight in zip(rnn_outputs, attention_weights)])\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_ws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def eval(model, test_set):\n",
    "    all_preds = []\n",
    "    for test_example in test_set:\n",
    "        doc, labels = test_example\n",
    "        labels = set()\n",
    "        \n",
    "        probs = get_probs(doc)\n",
    "        for i, prob in enumerate(probs):\n",
    "            prob = prob.value()\n",
    "            if prob[1] > prob[0]:\n",
    "                labels.add(i)\n",
    "        all_preds.append(labels)\n",
    "    \n",
    "    gold_y = []\n",
    "    for test_example in test_set:\n",
    "        doc, g = test_example\n",
    "        labels = set()\n",
    "        for i, l in enumerate(g):\n",
    "            if l > 0.5:\n",
    "                labels.add(i)\n",
    "        gold_y.append(labels)\n",
    "    \n",
    "    tp, fp, fn =0., 0., 0.      \n",
    "    for pred, gold in zip(all_preds, gold_y):\n",
    "        tp += len(pred.intersection(gold))\n",
    "        fp += len(pred-gold)\n",
    "        fn += len(gold-pred)\n",
    "    prec = tp/(tp+fp)\n",
    "    print(prec)\n",
    "    recal = tp/(tp+fn)\n",
    "    print(recal)\n",
    "    f = 2*(prec*recal)/(prec+recal)\n",
    "    print(f)\n",
    "    \n",
    "eval(model, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_important_words_and_sents(doc, i):\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    encoded_sents = []\n",
    "    words_attention_weights = []\n",
    "    for sent in doc:\n",
    "        words_attention_weights.append([])\n",
    "        \n",
    "        embedded_sent = [embeddings[word] for word in sent]\n",
    "\n",
    "        states =  word_gru_builder.initial_state().add_inputs(embedded_sent)\n",
    "        rnn_outputs = [s.output() for s in states]\n",
    "        \n",
    "        w = dy.parameter(word_attention_w1)\n",
    "        v = dy.parameter(word_attention_v)\n",
    "\n",
    "        attention_weights = [v*dy.tanh(w*o) for o in rnn_outputs]\n",
    "        attention_weights = dy.softmax(dy.concatenate(attention_weights))\n",
    "        \n",
    "        words_attention_weights[-1].append(attention_weights)\n",
    "        output_vector = dy.esum(\n",
    "            [vector * attention_weight for vector, attention_weight in zip(rnn_outputs, attention_weights)])\n",
    "        encoded_sents.append(output_vector)\n",
    "    \n",
    "    states = sent_gru_builder.initial_state().add_inputs(encoded_sents)\n",
    "    rnn_outputs = [s.output() for s in states]\n",
    "    \n",
    "    \n",
    "    w = dy.parameter(sent_attention_w1s[i])\n",
    "    v = dy.parameter(sent_attention_vs[i])\n",
    "    attention_weights = [v*dy.tanh(w*o) for o in rnn_outputs]\n",
    "    attention_weights = dy.softmax(dy.concatenate(attention_weights))\n",
    "\n",
    "    return attention_weights, words_attention_weights\n",
    "\n",
    "def print_words(x, sent_weights, words_attention_weights):\n",
    "    best_sent_index = np.argmax(sent_weights.npvalue())\n",
    "    best_word = np.argmax(words_attention_weights[best_sent_index][0].npvalue())\n",
    "    print('best sent:', ' '.join([all_words[word] for word in x[best_sent_index]]))\n",
    "    print('best word:', all_words[x[best_sent_index][best_word]])\n",
    "\n",
    "def analyze_example(x, y):\n",
    "    gold = set()\n",
    "    for i, l in enumerate(y):\n",
    "        if l > 0.5:\n",
    "            gold.add(i)\n",
    "    \n",
    "    predicted = set()  \n",
    "    probs = get_probs(x)\n",
    "    for i, prob in enumerate(probs):\n",
    "        prob = prob.value()\n",
    "        if prob[1] > prob[0]:\n",
    "            predicted.add(i)\n",
    "    \n",
    "    print('######TP######')\n",
    "    print(predicted & gold)\n",
    "    print([desc_dict[all_labels[label]] for label in predicted & gold])\n",
    "    print('######FP######')\n",
    "    print(predicted - gold)\n",
    "    print([desc_dict[all_labels[label]]for label in predicted - gold])\n",
    "    print('######FN######')\n",
    "    print(gold - predicted)\n",
    "    print([desc_dict[all_labels[label]]for label in gold - predicted])\n",
    "    print('---------')\n",
    "    for label in predicted | gold:\n",
    "        found = label in predicted \n",
    "        print('label:', desc_dict[all_labels[label]])\n",
    "        print('found:', found)\n",
    "        print('gold:', label in gold)\n",
    "        sent_weights, words_attention_weights = get_important_words_and_sents(x, label)\n",
    "        print_words(x, sent_weights, words_attention_weights)\n",
    "        print('---------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load('hagru_MIMIC2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_words = set()\n",
    "train_labels = set()\n",
    "for x, y in train_set:\n",
    "    for sent in x:\n",
    "        train_words |= set(sent)\n",
    "    for label, val in enumerate(y):\n",
    "        if val:\n",
    "            train_labels.add(label)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html/0.html\n",
      "html/1.html\n",
      "html/2.html\n",
      "html/3.html\n",
      "html/4.html\n",
      "html/5.html\n",
      "html/6.html\n",
      "html/7.html\n",
      "html/8.html\n",
      "html/9.html\n",
      "html/10.html\n",
      "html/11.html\n",
      "html/12.html\n"
     ]
    }
   ],
   "source": [
    "def get_sent_importance(doc):\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    encoded_sents = []\n",
    "    words_attention_weights = []\n",
    "    for sent in doc:\n",
    "        words_attention_weights.append([])\n",
    "        \n",
    "        embedded_sent = [embeddings[word] for word in sent]\n",
    "\n",
    "        states =  word_gru_builder.initial_state().add_inputs(embedded_sent)\n",
    "        rnn_outputs = [s.output() for s in states]\n",
    "        \n",
    "        w = dy.parameter(word_attention_w1)\n",
    "        v = dy.parameter(word_attention_v)\n",
    "\n",
    "        attention_weights = [v*dy.tanh(w*o) for o in rnn_outputs]\n",
    "        attention_weights = dy.softmax(dy.concatenate(attention_weights))\n",
    "        \n",
    "        words_attention_weights[-1].append(attention_weights.npvalue())\n",
    "    return words_attention_weights\n",
    "\n",
    "def get_hex_color(score):\n",
    "    scale = 1./4\n",
    "    decimal = int(255 * (1-((1-scale)*score+scale)))\n",
    "    hexa = (\"0x%0.2X\" % decimal)[-2:]\n",
    "    return '#'+hexa*3\n",
    "\n",
    "def pp_word(word):\n",
    "    if word in train_words:\n",
    "        return all_words[word]\n",
    "    return '<u>'+all_words[word]+'</u>'\n",
    "\n",
    "def gen_sents_html(x):\n",
    "    out_html = \"\\n\"\n",
    "\n",
    "    for i,(scores,sent) in enumerate(zip(get_sent_importance(x), x)):\n",
    "        scores = scores[0]\n",
    "        if len(sent) > 10:\n",
    "            out_html += \"<tr><td id=\\\"sent\"+str(i)+\"\\\"></td><td>\"\n",
    "        else:\n",
    "            out_html += \"<tr style=\\\"display:none;\\\"><td id=\\\"sent\"+str(i)+\"\\\"></td><td>\"\n",
    "        for score, word in zip(scores, sent):\n",
    "            out_html +=\"<font color=\\\"\"+get_hex_color(score)+\"\\\">\"+pp_word(word)+\" </font>\"\n",
    "        out_html+= \"</td></tr>\\n\"\n",
    "    return out_html\n",
    "\n",
    "def get_labels_types(x, y):\n",
    "    gold = set()\n",
    "    for i, l in enumerate(y):\n",
    "        if l > 0.5:\n",
    "            gold.add(i)\n",
    "    \n",
    "    predicted = set()  \n",
    "    probs = get_probs(x)\n",
    "    for i, prob in enumerate(probs):\n",
    "        prob = prob.value()\n",
    "        if prob[1] > prob[0]:\n",
    "            predicted.add(i)\n",
    "    \n",
    "    return list(predicted & gold), list(predicted - gold), list(gold - predicted)\n",
    "\n",
    "def get_sents_importance(doc, i):\n",
    "    dy.renew_cg()\n",
    "    \n",
    "    encoded_sents = []\n",
    "    for sent in doc:\n",
    "        \n",
    "        embedded_sent = [embeddings[word] for word in sent]\n",
    "\n",
    "        states =  word_gru_builder.initial_state().add_inputs(embedded_sent)\n",
    "        rnn_outputs = [s.output() for s in states]\n",
    "        \n",
    "        w = dy.parameter(word_attention_w1)\n",
    "        v = dy.parameter(word_attention_v)\n",
    "\n",
    "        attention_weights = [v*dy.tanh(w*o) for o in rnn_outputs]\n",
    "        attention_weights = dy.softmax(dy.concatenate(attention_weights))\n",
    "        \n",
    "        output_vector = dy.esum(\n",
    "            [vector * attention_weight for vector, attention_weight in zip(rnn_outputs, attention_weights)])\n",
    "        encoded_sents.append(output_vector)\n",
    "    \n",
    "    states = sent_gru_builder.initial_state().add_inputs(encoded_sents)\n",
    "    rnn_outputs = [s.output() for s in states]\n",
    "    \n",
    "    w = dy.parameter(sent_attention_w1s[i])\n",
    "    v = dy.parameter(sent_attention_vs[i])\n",
    "    attention_weights = [v*dy.tanh(w*o) for o in rnn_outputs]\n",
    "    attention_weights = dy.softmax(dy.concatenate(attention_weights))\n",
    "\n",
    "    return attention_weights.value()\n",
    "\n",
    "def pp_label(label):\n",
    "    if label in train_labels:\n",
    "        return desc_dict[all_labels[label]]\n",
    "    return '<u>'+desc_dict[all_labels[label]]+'</u>'\n",
    "    \n",
    "def gen_labels_html(x, y):\n",
    "    tp, fp, fn = get_labels_types(x, y)\n",
    "    label_to_id = {label:i for i, label in enumerate(tp+fp+fn)}\n",
    "    \n",
    "    tps_html = \"\"\n",
    "    for label in tp:\n",
    "        tps_html += \"<button class=\\\"btn btn-primary\\\" type=\\\"button\\\" onclick=\\\"myFunction(\"\n",
    "        tps_html += str(label_to_id[label])+\")\\\">\"\n",
    "        tps_html += pp_label(label)+\"</button>\\n\"\n",
    "    \n",
    "    fps_html = \"\"\n",
    "    for label in fp:\n",
    "        fps_html += \"<button class=\\\"btn btn-primary\\\" type=\\\"button\\\" onclick=\\\"myFunction(\"\n",
    "        fps_html += str(label_to_id[label])+\")\\\">\"\n",
    "        fps_html += pp_label(label)+\"</button>\\n\"\n",
    "        \n",
    "    fns_html = \"\"\n",
    "    for label in fn:\n",
    "        fns_html += \"<button class=\\\"btn btn-primary\\\" type=\\\"button\\\" onclick=\\\"myFunction(\"\n",
    "        fns_html += str(label_to_id[label])+\")\\\">\"\n",
    "        fns_html += pp_label(label)+\"</button>\\n\"\n",
    "        \n",
    "    label_names_html = str([desc_dict[all_labels[label]] for label in tp+fp+fn])+'\\n'\n",
    "    weights_list_html = str([get_sents_importance(x, label) for label in tp+fp+fn])+'\\n'\n",
    "    \n",
    "    return tps_html ,fps_html ,fns_html, label_names_html,weights_list_html\n",
    "    \n",
    "def gen_visualize_html(x, y, i):\n",
    "    html = ' '.join(open('visualizer_html_template').readlines())\n",
    "    sents_html = gen_sents_html(x)\n",
    "    tps_html ,fps_html ,fns_html, label_names_html, weights_list_html = gen_labels_html(x,y)\n",
    "    \n",
    "    html = html.replace('###WEIGHTS###', weights_list_html)\n",
    "    html = html.replace('###LABEL_NAMES###', label_names_html)\n",
    "    html = html.replace('###SENTS###', sents_html)\n",
    "    html = html.replace('###TPS###', tps_html)\n",
    "    html = html.replace('###FPS###', fps_html)\n",
    "    html = html.replace('###FNS###', fns_html)\n",
    "    html = html.replace('###PREV###', str(i-2)+'.html')\n",
    "    html = html.replace('###NEXT###', str(i)+'.html')\n",
    "    return html\n",
    "\n",
    "for i, item in enumerate(test_set):\n",
    "    html = gen_visualize_html(item[0], item[1], i+1)\n",
    "    print('html/'+str(i)+'.html')\n",
    "    with open('html/'+str(i)+'.html', 'w',encoding='utf-8') as fp:\n",
    "        fp.write(html)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
