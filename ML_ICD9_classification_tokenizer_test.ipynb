{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for ICD9 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIMIC 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/datasets/MIMIC/mimic_code')\n",
    "\n",
    "from jumana.MIMIC_reader import MIMIC_Reader\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from jumana.evaluate_predictions import Evaluation\n",
    "from jumana.icd92int import *\n",
    "from jumana.MIMIC_data import MIMIC_Data\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "mimic_data = MIMIC_Data('2')\n",
    "corpus_path = mimic_data.get_mimic_preprocessed_path()\n",
    "\n",
    "#loading training and testing data using MIMIc_reader \n",
    "data_train = MIMIC_Reader(mimic_data,is_train=True)\n",
    "data_test = MIMIC_Reader(mimic_data,is_train=False)\n",
    "categories = data_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = []\n",
    "import codecs\n",
    "for line in codecs.open('mimic_code/tal/shaped_mimic2', encoding='utf-8'):\n",
    "    tokenized_corpus.append(line.strip())\n",
    "data_train.data = tokenized_corpus[:len(data_train.data)]\n",
    "data_test.data = tokenized_corpus[len(data_train.data):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the labels from text to a 2d binary matrix, (that's what the classifiers accept)\n",
    "lb = preprocessing.MultiLabelBinarizer()\n",
    "data_train.target = lb.fit_transform(data_train.target);\n",
    "\n",
    "#remove labels from the test set that don't appear in the train set.\n",
    "test_actual_targets = []\n",
    "for tag_set in data_test.target:\n",
    "    test_actual_targets.append([tag for tag in tag_set if tag in lb.classes_])\n",
    "data_test.target = lb.transform(test_actual_targets)\n",
    "\n",
    "#setting training targets and testing targets\n",
    "y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "print('data loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing statstics about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
    "\n",
    "data_train_size_mb = size_mb(data_train.data)\n",
    "data_test_size_mb = size_mb(data_test.data)\n",
    "\n",
    "print(\"%d documents - %0.3fMB (training set)\" % (len(data_train.data), data_train_size_mb))\n",
    "print(\"%d documents - %0.3fMB (test set)\" % (len(data_test.data), data_test_size_mb))\n",
    "print(\"%d categories\\n\" % len(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting tf-idf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#used to check how much time feature extraction took\n",
    "t0 = time()\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_features=10000,stop_words='english', ngram_range = (1,3))\n",
    "X_train = vectorizer.fit_transform(data_train.data)\n",
    "\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\\n\" % X_train.shape)\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "t0 = time()\n",
    "X_test = vectorizer.transform(data_test.data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_test_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\\n\" % X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation is done using Perotte's code from the heirarchy SVM paper\n",
    "def  evaluate_predictions(pred_binary,name):\n",
    "    predictions = lb.inverse_transform(pred_binary)\n",
    "    icd2int = init(mimic_data)\n",
    "    #map labels to int using Perotte's mapping file the list contains indices represented in a string format\n",
    "    predictions = icd92int(predictions,icd2int)\n",
    "    #convert the strings to ints\n",
    "    predictions_int = []\n",
    "    for prediction_set in predictions:\n",
    "        predictions_int.append([int(x) for x in prediction_set])\n",
    "    evaluation = Evaluation(mimic_data)\n",
    "    evaluation.evaluate(predictions_int,name + '.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def benchmark(clf,name):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "    \n",
    "    #evaluation using Perotte's \n",
    "    evaluate_predictions(pred,name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#logistic regression,chi2+kbest,tf-idf \n",
    "clf_log_chi = Pipeline([('chi2', SelectKBest(chi2, k=10000)),#opts.select_chi2 )),\n",
    "                ('logistic_regression', LogisticRegression(n_jobs=3))])\n",
    "#linear svc, chi2+kbest, tf-idf\n",
    "clf_svc_chi = Pipeline([('chi2', SelectKBest(chi2, k=10000)),#opts.select_chi2 )), #try 20k 30k on tfidf and chi see if it makes difference\n",
    "                ('linear_svc', LinearSVC())])\n",
    "\n",
    "for clf, name in (\n",
    "        #linear svc, tf-idf\n",
    "        (OneVsRestClassifier(LinearSVC(),n_jobs=3), 'linear_svc1'), #l2 hinge_squared\n",
    "        (OneVsRestClassifier(clf_log_chi,n_jobs=3), 'logistic_regression_chi_tfidf'),\n",
    "        #the resutls were exactly the same as the results in linear_svc without chi, try later to play with the \n",
    "        #number of features of chi and tf-idf\n",
    "        (OneVsRestClassifier(clf_svc_chi,n_jobs=3), 'svc_chi_tfidf')\n",
    "):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    benchmark(clf,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}