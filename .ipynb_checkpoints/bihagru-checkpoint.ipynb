{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "sys.argv.append('--dynet_mem')\n",
    "sys.argv.append('6000')\n",
    "#sys.argv.append('--dynet-autobatch')\n",
    "#sys.argv.append('1')\n",
    "import _dynet as dy\n",
    "from full_eval import full_evaluate\n",
    "dy.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = '3'\n",
    "with open('mimic/'+v+'_all_labels.p',mode='rb') as fp:\n",
    "#with open('mimic/'+v+'_all_labels.p',mode='rb') as fp:\n",
    "    x,y,all_words, = pickle.load(fp)\n",
    "\n",
    "    \n",
    "desc_dict = {}\n",
    "for line in open('mimic/2/ICD9_descriptions'):\n",
    "    line = line.strip().split('\\t')\n",
    "    desc_dict[line[0]] = line[1]\n",
    "    \n",
    "train_set = list(zip(x[:-2282], y[:-2282]))\n",
    "test_set = list(zip(x[-2282:], y[-2282:]))\n",
    "VOCAB_SIZE = len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6527"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_size = 128\n",
    "word_gru_layers = 1\n",
    "word_gru_state_size = 128\n",
    "sent_gru_layers = 1\n",
    "sent_gru_state_size = 128\n",
    "dropout_p = 0.1\n",
    "\n",
    "\n",
    "model = dy.Model()\n",
    "\n",
    "embeddings = model.add_lookup_parameters((VOCAB_SIZE, embeddings_size))\n",
    "word_gru_builder_fw = dy.GRUBuilder(word_gru_layers, embeddings_size, word_gru_state_size, model)\n",
    "word_gru_builder_bw = dy.GRUBuilder(word_gru_layers, embeddings_size, word_gru_state_size, model)\n",
    "word_attention_w1 = model.add_parameters((word_gru_state_size, word_gru_state_size*2))\n",
    "word_attention_v = model.add_parameters((1, word_gru_state_size))\n",
    "\n",
    "sent_gru_builder_fw = dy.GRUBuilder(sent_gru_layers, word_gru_state_size*2, sent_gru_state_size, model)\n",
    "sent_gru_builder_bw = dy.GRUBuilder(sent_gru_layers, word_gru_state_size*2, sent_gru_state_size, model)\n",
    "\n",
    "sent_attention_w1s = []\n",
    "sent_attention_vs = []\n",
    "classifier_ws = []\n",
    "classifier_bs = []\n",
    "\n",
    "for _ in all_labels:\n",
    "    sent_attention_w1s.append(model.add_parameters((sent_gru_state_size, sent_gru_state_size*2)))\n",
    "    #word_w2 =  \n",
    "    sent_attention_vs.append(model.add_parameters((1, sent_gru_state_size)))\n",
    "    \n",
    "    classifier_ws.append(model.add_parameters((2, sent_gru_state_size*2)))\n",
    "    classifier_bs.append(model.add_parameters((2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_probs(doc, is_train=False):\n",
    "    def run_birnn(input_seq, fw, bw):\n",
    "        input_seq_rev = input_seq[::-1]\n",
    "        fw_output = fw.initial_state().transduce(input_seq)\n",
    "        bw_output = bw.initial_state().transduce(input_seq_rev)[::-1]\n",
    "        birnn_out = [dy.concatenate([fw_o, bw_o]) for fw_o, bw_o in zip(fw_output, bw_output)]\n",
    "        if is_train:\n",
    "            birnn_out = [dy.dropout(exp, dropout_p) for exp in birnn_out]\n",
    "        return birnn_out\n",
    "    \n",
    "    def attend(rnn_outs, w, v):\n",
    "        w = dy.parameter(w)\n",
    "        v = dy.parameter(v)\n",
    "\n",
    "        attention_weights = [v*dy.tanh(w*o) for o in rnn_outs]\n",
    "        attention_weights = dy.softmax(dy.concatenate(attention_weights))\n",
    "        \n",
    "        output_vector = dy.esum(\n",
    "            [vector * attention_weight for vector, attention_weight in zip(rnn_outs, attention_weights)])\n",
    "        if is_train:\n",
    "            output_vector = dy.dropout(output_vector, dropout_p)\n",
    "        return output_vector\n",
    "        \n",
    "        \n",
    "    encoded_sents = []\n",
    "    for sent in doc:\n",
    "        embedded_sent = [embeddings[word] for word in sent]\n",
    "        if is_train:\n",
    "            embedded_sent = [dy.dropout(exp, dropout_p) for exp in embedded_sent]\n",
    "            \n",
    "        rnn_outputs = run_birnn(embedded_sent, word_gru_builder_fw, word_gru_builder_bw)\n",
    "        output_vector = attend(rnn_outputs, word_attention_w1, word_attention_v)\n",
    "        encoded_sents.append(output_vector)\n",
    "    \n",
    "    rnn_outputs = run_birnn(encoded_sents, sent_gru_builder_fw, sent_gru_builder_bw)\n",
    "    \n",
    "    all_probs = []\n",
    "    for i in range(len(all_labels)):\n",
    "        output_vector = attend(rnn_outputs, sent_attention_w1s[i], sent_attention_vs[i])\n",
    "        \n",
    "        w = dy.parameter(classifier_ws[i])\n",
    "        b = dy.parameter(classifier_bs[i])\n",
    "        probs = dy.softmax(w * output_vector + b)\n",
    "        all_probs.append(probs)\n",
    "    return all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_set, epochs = 20):\n",
    "    def get_loss(probs, trues):\n",
    "        return dy.esum([-dy.log(dy.pick(prob, true)) for prob, true in zip(probs, trues)])\n",
    "    \n",
    "    trainer = dy.AdamTrainer(model)\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        batch_size = 1\n",
    "        for i in tqdm_notebook(range(0, len(train_set), batch_size)):\n",
    "            \n",
    "            dy.renew_cg()\n",
    "            losses = []\n",
    "            for training_example in train_set[i:i+batch_size]:\n",
    "                doc, labels = training_example\n",
    "\n",
    "                loss = get_loss(get_probs(doc, True), labels)\n",
    "                losses.append(loss)\n",
    "            batch_loss = dy.esum(losses)\n",
    "            batch_loss.npvalue() # this calls forward on the batch\n",
    "            batch_loss.backward()\n",
    "            trainer.update()\n",
    "\n",
    "\n",
    "        print('epoch', e, 'done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval(model, test_set):\n",
    "    all_probs = []\n",
    "    for test_example in tqdm_notebook(test_set):\n",
    "        doc, labels = test_example\n",
    "        labels = set()\n",
    "        dy.renew_cg()\n",
    "        probs = get_probs(doc)\n",
    "        all_probs.append([p.value()[1] for p in probs])\n",
    "        \n",
    "    gold_y = []\n",
    "    for test_example in test_set:\n",
    "        doc, g = test_example\n",
    "        gold_y.append(g)\n",
    "        \n",
    "    return full_evaluate(all_probs, gold_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97271b1da25643cc83daef4bed8bb6b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 0 done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968f8b01698c4a538f07397f96177a5e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388d023353274f318436d45e55f847a0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 0 done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ab32c8ed4b4541b270292bebe7a94e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b69f12be53e43249e9c499e7fc45c83"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    train(model, train_set, 1)\n",
    "    s = 'ha-bigru rolled labels (larger) implemented in dynet '+str(i+1)+' epochs MIMIC-'+v+'\\n'+eval(model, test_set)\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
